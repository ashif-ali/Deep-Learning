{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>92</td>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>37</td>\n",
       "      <td>5.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>41</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>52</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>66</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>86</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0    126            7.2           3.2            6.0           1.8   \n",
       "1     44            5.0           3.5            1.6           0.6   \n",
       "2     92            6.1           3.0            4.6           1.4   \n",
       "3     98            6.2           2.9            4.3           1.3   \n",
       "4    137            6.3           3.4            5.6           2.4   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "145   37            5.5           3.5            1.3           0.2   \n",
       "146   41            5.0           3.5            1.3           0.3   \n",
       "147   52            6.4           3.2            4.5           1.5   \n",
       "148   66            6.7           3.1            4.4           1.4   \n",
       "149   86            6.0           3.4            4.5           1.6   \n",
       "\n",
       "             Species  \n",
       "0     Iris-virginica  \n",
       "1        Iris-setosa  \n",
       "2    Iris-versicolor  \n",
       "3    Iris-versicolor  \n",
       "4     Iris-virginica  \n",
       "..               ...  \n",
       "145      Iris-setosa  \n",
       "146      Iris-setosa  \n",
       "147  Iris-versicolor  \n",
       "148  Iris-versicolor  \n",
       "149  Iris-versicolor  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv(\"Iris.csv\")\n",
    "# randomize the data\n",
    "iris = iris.sample(frac=1).reset_index(drop=True)\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.2, 3.2, 6. , 1.8],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [6.3, 3.4, 5.6, 2.4]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris[['SepalLengthCm','SepalWidthCm', 'PetalLengthCm','PetalWidthCm']]\n",
    "# converting into numpy array\n",
    "X = np.array(X)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot_encode = pd.concat([iris, pd.get_dummies(iris['Species'], prefix = 'Species')], axis = 1)\n",
    "# hot_encode = hot_encode.drop('Species', axis = 1)\n",
    "# hot_encode[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashif\\anaconda3\\envs\\python_3.12\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an encoder to convert categorical labels to one-hot encoded vectors\n",
    "one_hot_encoder = OneHotEncoder(sparse = False)  # Explicitly set sparse to False for dense output\n",
    "\n",
    "# Extract the species labels from the iris dataset\n",
    "Y = iris.Species\n",
    "\n",
    "# Learn the unique labels and transform the data into one-hot encoded vectors\n",
    "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1,1))\n",
    "\n",
    "# Print the first 5 rows of the encoded labels\n",
    "Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### spliting data into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights\n",
    "\n",
    "**1. Function Definition:**\n",
    "\n",
    "- `def initialize_weights(node_counts):`: Defines a function named `initialize_weights` that takes a list of node counts as input.\n",
    "\n",
    "**2. Function Purpose:**\n",
    "\n",
    "- The function initializes weights for a neural network with random values between -1 and 1.\n",
    "\n",
    "**3. Arguments:**\n",
    "\n",
    "- `node_counts`: A list containing the number of nodes in each layer of the neural network.\n",
    "\n",
    "**4. Returns:**\n",
    "\n",
    "- A list of weight matrices, one for each layer (except the input layer), representing the connections between nodes in adjacent layers.\n",
    "\n",
    "**5. Code Steps:**\n",
    "\n",
    "- **Calculate Number of Layers:**\n",
    "   - `number_of_layers = len(node_counts)`: Determines the total number of layers in the network based on the length of the `node_counts` list.\n",
    "- **Initialize List for Weights:**\n",
    "   - `weights = []`: Creates an empty list to store the generated weight matrices.\n",
    "- **Loop Through Layers (Except Input):**\n",
    "   - `for layer_index in range(1, number_of_layers):`: Iterates through each layer, starting from the second layer (index 1) to avoid the input layer.\n",
    "      - `current_layer_nodes = node_counts[layer_index]`: Retrieves the number of nodes in the current layer.\n",
    "      - `previous_layer_nodes = node_counts[layer_index - 1]`: Retrieves the number of nodes in the previous layer.\n",
    "      - **Create Weight Matrix:**\n",
    "         - `weight_matrix = np.random.uniform(-1, 1, size=(current_layer_nodes, previous_layer_nodes + 1))`: Creates a matrix of random values between -1 and 1, with dimensions matching the number of nodes in the current and previous layers. The extra column is for bias weights.\n",
    "         - `weight_matrix = np.matrix(weight_matrix)`: Converts the matrix to a NumPy matrix for compatibility.\n",
    "      - **Append Weight Matrix:**\n",
    "         - `weights.append(weight_matrix)`: Adds the generated weight matrix to the `weights` list.\n",
    "- **Return Weights:**\n",
    "   - `return weights`: Returns the list of weight matrices as the function's output.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- This function is essential for initializing weights in neural networks before training.\n",
    "- It ensures that weights start with random values to avoid bias and allow the network to learn during training.\n",
    "- The weights are crucial for determining the strength of connections between nodes and how signals propagate through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(node_counts):\n",
    "    \"\"\"\n",
    "    Initializes weights for a neural network with random values between -1 and 1.\n",
    "\n",
    "    Args:\n",
    "        node_counts (list): A list containing the number of nodes in each layer.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of weight matrices, one for each layer (except the input layer).\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_layers = len(node_counts)\n",
    "    weights = []\n",
    "\n",
    "    # Create weight matrices for each layer (except the input layer)\n",
    "    for layer_index in range(1, number_of_layers):\n",
    "        current_layer_nodes = node_counts[layer_index]\n",
    "        previous_layer_nodes = node_counts[layer_index - 1]\n",
    "\n",
    "        # Initialize weights with random values\n",
    "        weight_matrix = np.random.uniform(-1, 1, size=(current_layer_nodes, previous_layer_nodes + 1))\n",
    "\n",
    "        # Convert to a NumPy matrix for compatibility\n",
    "        weight_matrix = np.matrix(weight_matrix)\n",
    "\n",
    "        weights.append(weight_matrix)\n",
    "        \n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe weight matrix will look something like this:\\n\\nWeight Matrix 1: (6, 5)\\n[[ 0.123  -0.456   0.789   0.321   0.234 ] \\n [ 0.567  -0.678  -0.123   0.876   0.456 ]\\n [ 0.789   0.987  -0.234  -0.567  -0.789 ]\\n [-0.987   0.654   0.321  -0.432   0.012 ]\\n [ 0.345  -0.789   0.876   0.234  -0.567 ]\\n [ 0.123   0.456  -0.678   0.789   0.321 ]]\\n\\nWeight Matrix 2: (9, 6)\\n[[ 0.234  -0.567   0.789   0.432   0.012   0.876   0.789  -0.123   0.567 ]\\n [-0.789   0.345  -0.876   0.234   0.567  -0.678  -0.987   0.321  -0.234 ]\\n [ 0.876   0.123   0.456   0.789   0.321   0.234  -0.567  -0.789   0.987 ]\\n [ 0.567  -0.678   0.789  -0.987   0.654   0.321  -0.432   0.012   0.345 ]\\n [-0.234   0.789  -0.321   0.876  -0.789   0.234   0.456   0.678  -0.876 ]\\n [ 0.987  -0.432  -0.567   0.012   0.345  -0.678   0.789  -0.123   0.567 ]\\n [ 0.456  -0.876   0.234  -0.567   0.789   0.321  -0.234   0.987  -0.789 ]\\n [ 0.321   0.234  -0.567   0.789   0.987  -0.234   0.456  -0.789   0.876 ]\\n [-0.678   0.789   0.012   0.345  -0.876   0.567   0.234  -0.789   0.123 ]]\\n\\nWeight Matrix 3: (3, 9)\\n[[ 0.987  -0.234   0.456   0.789  -0.321   0.012   0.345  -0.678   0.876 ]\\n [ 0.567   0.789  -0.876   0.234   0.321  -0.987  -0.678   0.123   0.456 ]\\n [-0.789   0.234   0.567  -0.876   0.987   0.012   0.345  -0.678   0.876 ]]\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The weight matrix will look something like this:\n",
    "\n",
    "Weight Matrix 1: (6, 5)\n",
    "[[ 0.123  -0.456   0.789   0.321   0.234 ] \n",
    " [ 0.567  -0.678  -0.123   0.876   0.456 ]\n",
    " [ 0.789   0.987  -0.234  -0.567  -0.789 ]\n",
    " [-0.987   0.654   0.321  -0.432   0.012 ]\n",
    " [ 0.345  -0.789   0.876   0.234  -0.567 ]\n",
    " [ 0.123   0.456  -0.678   0.789   0.321 ]]\n",
    "\n",
    "Weight Matrix 2: (9, 6)\n",
    "[[ 0.234  -0.567   0.789   0.432   0.012   0.876   0.789  -0.123   0.567 ]\n",
    " [-0.789   0.345  -0.876   0.234   0.567  -0.678  -0.987   0.321  -0.234 ]\n",
    " [ 0.876   0.123   0.456   0.789   0.321   0.234  -0.567  -0.789   0.987 ]\n",
    " [ 0.567  -0.678   0.789  -0.987   0.654   0.321  -0.432   0.012   0.345 ]\n",
    " [-0.234   0.789  -0.321   0.876  -0.789   0.234   0.456   0.678  -0.876 ]\n",
    " [ 0.987  -0.432  -0.567   0.012   0.345  -0.678   0.789  -0.123   0.567 ]\n",
    " [ 0.456  -0.876   0.234  -0.567   0.789   0.321  -0.234   0.987  -0.789 ]\n",
    " [ 0.321   0.234  -0.567   0.789   0.987  -0.234   0.456  -0.789   0.876 ]\n",
    " [-0.678   0.789   0.012   0.345  -0.876   0.567   0.234  -0.789   0.123 ]]\n",
    "\n",
    "Weight Matrix 3: (3, 9)\n",
    "[[ 0.987  -0.234   0.456   0.789  -0.321   0.012   0.345  -0.678   0.876 ]\n",
    " [ 0.567   0.789  -0.876   0.234   0.321  -0.987  -0.678   0.123   0.456 ]\n",
    " [-0.789   0.234   0.567  -0.876   0.987   0.012   0.345  -0.678   0.876 ]]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return np.multiply(x, 1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "\n",
    "**1. Function Definition:**\n",
    "\n",
    "- `def feed_forward(inputs, weights, number_of_layers):`: Defines a function named `feed_forward` that simulates the feedforward process in a neural network.\n",
    "\n",
    "**2. Function Purpose:**\n",
    "\n",
    "- The function calculates the activations of each layer in a neural network, given input values and weight matrices.\n",
    "\n",
    "**3. Arguments:**\n",
    "\n",
    "- `inputs`: The input values provided to the network.\n",
    "- `weights`: A list of weight matrices representing the connections between layers.\n",
    "- `number_of_layers`: The total number of layers in the network.\n",
    "\n",
    "**4. Returns:**\n",
    "\n",
    "- A list of activation values for each layer, reflecting the output of each layer's neurons during the feedforward process.\n",
    "\n",
    "**5. Code Steps:**\n",
    "\n",
    "- **Initialize Variables:**\n",
    "   - `activations = [inputs]`: Creates a list to store the activations of each layer, starting with the input values as the activations of the first layer.\n",
    "   - `current_input = inputs`: Sets the initial input for the first layer.\n",
    "- **Iterate Through Layers:**\n",
    "   - `for layer_index in range(number_of_layers):`: Iterates through each layer in the network.\n",
    "      - `current_weights = weights[layer_index]`: Retrieves the weight matrix for the current layer.\n",
    "      - **Calculate Layer Activation:**\n",
    "         - `layer_activation = sigmoid(np.dot(current_input, current_weights.T))`:\n",
    "           - Calculates the weighted sum of inputs and weights using matrix multiplication (`np.dot`).\n",
    "           - Applies the sigmoid activation function to the weighted sum, resulting in the activation values for the current layer's neurons.\n",
    "      - **Append Bias Term:**\n",
    "         - `layer_input_with_bias = np.append(1, layer_activation)`: Adds a bias term (1) to the activations, preparing them as input for the next layer.\n",
    "      - **Store Activations and Prepare for Next Layer:**\n",
    "         - `activations.append(layer_activation)`: Stores the calculated activations for the current layer in the `activations` list.\n",
    "         - `current_input = layer_input_with_bias`: Updates the `current_input` for the next iteration of the loop, using the bias-appended activations as input for the subsequent layer.\n",
    "- **Return Activations:**\n",
    "   - `return activations`: Returns the list of activations for each layer, representing the network's output during the feedforward process.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- This function is central to the forward propagation step in neural network training.\n",
    "- It simulates how signals flow through the network, from input to output, through sequential activations of neurons in each layer.\n",
    "- Understanding this process is essential for comprehending how neural networks process information and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(inputs, weights, number_of_layers):\n",
    "    \"\"\"\n",
    "    Calculates the activations of each layer in a neural network during the feedforward process.\n",
    "\n",
    "    Args:\n",
    "        inputs (array-like): The input values to the network.\n",
    "        weights (list): A list of weight matrices for each layer.\n",
    "        number_of_layers (int): The total number of layers in the network.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of activation values for each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    activations = [inputs]  # Store activations for each layer\n",
    "    current_input = inputs  # Initialize input for the first layer\n",
    "\n",
    "    for layer_index in range(number_of_layers):\n",
    "        current_weights = weights[layer_index]\n",
    "\n",
    "        # Calculate weighted sum and apply activation function\n",
    "        layer_activation = sigmoid(np.dot(current_input, current_weights.T))\n",
    "\n",
    "        # Append bias term for the next layer\n",
    "        layer_input_with_bias = np.append(1, layer_activation)\n",
    "\n",
    "        activations.append(layer_activation)  # Store activations\n",
    "        current_input = layer_input_with_bias  # Prepare input for the next layer\n",
    "\n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActivation list will look something like this:\\n\\nActivation Layer 1: [0.1 0.2 0.3 0.4 0.5]\\n\\nActivation Layer 2: [0.793 0.559 0.853 0.832 0.542 0.802]\\n\\nActivation Layer 3: [0.742 0.675 0.733 0.723 0.778 0.676 0.669 0.654 0.778]\\n\\nActivation Layer 4: [0.831 0.756 0.681]\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Activation list will look something like this:\n",
    "\n",
    "Activation Layer 1: [0.1 0.2 0.3 0.4 0.5]\n",
    "\n",
    "Activation Layer 2: [0.793 0.559 0.853 0.832 0.542 0.802]\n",
    "\n",
    "Activation Layer 3: [0.742 0.675 0.733 0.723 0.778 0.676 0.669 0.654 0.778]\n",
    "\n",
    "Activation Layer 4: [0.831 0.756 0.681]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Here's a breakdown of the code:**\n",
    "\n",
    "**1. Import:**\n",
    "\n",
    "- `import numpy as np`: Imports the NumPy library for numerical operations and array manipulation.\n",
    "\n",
    "**2. Function Definition:**\n",
    "\n",
    "- `def back_propagation(target_values, activations, weights, number_of_layers, learning_rate):`: Defines a function named `back_propagation` that implements the backpropagation algorithm for weight updates in neural networks.\n",
    "\n",
    "**3. Function Purpose:**\n",
    "\n",
    "- The function adjusts the weights of a neural network based on the errors observed during the feedforward process, aiming to improve its performance over time.\n",
    "\n",
    "**4. Arguments:**\n",
    "\n",
    "- `target_values`: The desired output values that the network should have produced.\n",
    "- `activations`: A list of activation values for each layer, calculated during the feedforward process.\n",
    "- `weights`: A list of weight matrices representing the connections between layers, initially generated randomly.\n",
    "- `number_of_layers`: The total number of layers in the network.\n",
    "- `learning_rate`: A hyperparameter that controls the magnitude of weight updates in each iteration.\n",
    "\n",
    "**5. Returns:**\n",
    "\n",
    "- A list of updated weight matrices, reflecting the adjustments made based on the backpropagation calculations.\n",
    "\n",
    "**6. Code Steps:**\n",
    "\n",
    "- **Calculate Output Error:**\n",
    "   - `output_layer_activations = activations[-1]`: Accesses the activations of the output layer from the `activations` list.\n",
    "   - `output_error = np.matrix(target_values - output_layer_activations)`: Calculates the difference between the target values and the actual output, representing the error at the output layer.\n",
    "\n",
    "- **Iterate Through Layers in Reverse:**\n",
    "   - `for layer_index in reversed(range(number_of_layers)):`: Iterates through the layers of the network in reverse order, starting from the output layer and moving towards the input layer.\n",
    "      - `current_layer_activations = activations[layer_index]`: Retrieves the activations of the current layer.\n",
    "      - **Prepare Previous Layer's Activations:**\n",
    "         - Handles the bias node for proper calculations.\n",
    "      - **Calculate Error Gradient (Delta):**\n",
    "         - `layer_delta = np.multiply(output_error, sigmoid_derivative(current_layer_activations))`: Computes the error gradient (delta) for the current layer, involving the output error and the derivative of the activation function.\n",
    "      - **Update Weights:**\n",
    "         - `weights[layer_index - 1] += learning_rate * np.multiply(layer_delta.T, previous_layer_activations)`: Updates the weights connecting the current layer to the previous layer, using the learning rate and the product of the error gradient and previous layer's activations.\n",
    "      - **Remove Bias Weight for Error Propagation:**\n",
    "         - `weights_without_bias = np.delete(weights[layer_index - 1], [0], axis=1)`: Removes the bias weight column for error propagation to previous layers.\n",
    "         - `output_error = np.dot(layer_delta, weights_without_bias)`: Propagates the error back to the previous layer, calculating the error for the previous layer's neurons.\n",
    "\n",
    "**7. Return Updated Weights:**\n",
    "   - `return weights`: Returns the list of updated weight matrices after all layers have been processed.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- This function is a crucial part of the training process in neural networks.\n",
    "- It iteratively adjusts the weights to minimize errors and improve the network's ability to map inputs to desired outputs.\n",
    "- Understanding backpropagation is essential for comprehending how neural networks learn and adapt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Explicitly import NumPy\n",
    "\n",
    "def back_propagation(target_values, activations, weights, number_of_layers, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs the backpropagation algorithm to update weights in a neural network.\n",
    "\n",
    "    Args:\n",
    "        target_values (array-like): The desired output values for the network.\n",
    "        activations (list): A list of activation values for each layer.\n",
    "        weights (list): A list of weight matrices for each layer.\n",
    "        number_of_layers (int): The total number of layers in the network.\n",
    "        learning_rate (float): The learning rate for weight updates.\n",
    "\n",
    "    Returns:\n",
    "        list: The updated weight matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    output_layer_activations = activations[-1]  # Access output layer activations\n",
    "    output_error = np.matrix(target_values - output_layer_activations)  # Calculate output error\n",
    "\n",
    "    for layer_index in range(number_of_layers, 0, -1):  # Iterate through layers in reverse\n",
    "        current_layer_activations = activations[layer_index]\n",
    "\n",
    "        # Prepare previous layer's activations (considering bias)\n",
    "        if layer_index > 1:\n",
    "            previous_layer_activations = np.append(1, activations[layer_index - 1])\n",
    "        else:\n",
    "            previous_layer_activations = activations[0]\n",
    "\n",
    "        # Calculate error gradient (delta)\n",
    "        layer_delta = np.multiply(output_error, sigmoid_derivative(current_layer_activations))\n",
    "\n",
    "        # Update weights\n",
    "        weights[layer_index - 1] += learning_rate * np.multiply(layer_delta.T, previous_layer_activations)\n",
    "\n",
    "        # Remove bias weight for error propagation to previous layers\n",
    "        weights_without_bias = np.delete(weights[layer_index - 1], [0], axis=1)\n",
    "        output_error = np.dot(layer_delta, weights_without_bias)  # Propagate error back\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(input_data, target_values, learning_rate, initial_weights):\n",
    "    \"\"\"\n",
    "    Trains a neural network using the feedforward and backpropagation algorithms.\n",
    "\n",
    "    Args:\n",
    "        input_data (array-like): The input data for training.\n",
    "        target_values (array-like): The desired output values for each input.\n",
    "        learning_rate (float): The learning rate for weight updates.\n",
    "        initial_weights (list): A list of initial weight matrices.\n",
    "\n",
    "    Returns:\n",
    "        list: The trained weight matrices.\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_layers = len(initial_weights)  # Determine the number of layers\n",
    "\n",
    "    for i in range(len(input_data)):\n",
    "        current_input = input_data[i]\n",
    "        target_output = target_values[i]\n",
    "\n",
    "        # Prepare input with bias term\n",
    "        input_with_bias = np.append(1, current_input)\n",
    "        input_matrix = np.matrix(input_with_bias)\n",
    "\n",
    "        # Perform feedforward pass\n",
    "        layer_activations = feed_forward(input_matrix, initial_weights, number_of_layers)\n",
    "\n",
    "        # Backpropagate errors and update weights\n",
    "        updated_weights = back_propagation(target_output, layer_activations, initial_weights, number_of_layers, learning_rate)\n",
    "\n",
    "    return updated_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(input_item, trained_weights):\n",
    "    \"\"\"\n",
    "    Predicts the output for a given input using a trained neural network.\n",
    "\n",
    "    Args:\n",
    "        input_item (array-like): The input values to be processed.\n",
    "        trained_weights (list): The trained weight matrices of the network.\n",
    "\n",
    "    Returns:\n",
    "        list: A list representing the predicted output, with a 1 in the position of the highest predicted value.\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_layers = len(trained_weights)  # Determine the number of layers\n",
    "\n",
    "    # Add bias term to input\n",
    "    input_with_bias = np.append(1, input_item)\n",
    "\n",
    "    # Perform feedforward pass\n",
    "    layer_activations = feed_forward(input_with_bias, trained_weights, number_of_layers)\n",
    "\n",
    "    # Extract output layer activations\n",
    "    output_activations = layer_activations[-1].A1\n",
    "\n",
    "    # Find index of the highest activation\n",
    "    highest_activation_index = np.argmax(output_activations)\n",
    "\n",
    "    # Create predicted output list with a 1 at the index of the highest activation\n",
    "    predicted_output = [0 for _ in range(len(output_activations))]\n",
    "    predicted_output[highest_activation_index] = 1\n",
    "\n",
    "    return predicted_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(input_data, target_labels, trained_weights):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of a trained neural network on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        input_data (array-like): The input data to evaluate.\n",
    "        target_labels (array-like): The correct labels for each input example.\n",
    "        trained_weights (list): The trained weight matrices of the network.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the provided dataset, as a percentage.\n",
    "    \"\"\"\n",
    "\n",
    "    total_examples = len(input_data)\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(total_examples):\n",
    "        current_input = input_data[i]\n",
    "        target_label = list(target_labels[i])  # Ensure target label is a list\n",
    "\n",
    "        predicted_output = predict_output(current_input, trained_weights)\n",
    "\n",
    "        if predicted_output == target_label:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_examples\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # Explicitly import NumPy\n",
    "\n",
    "def neural_network(train_data, train_labels, validation_data=None, validation_labels=None,\n",
    "                         number_of_epochs=10, layer_nodes=[], learning_rate=0.15):\n",
    "    \"\"\"\n",
    "    Trains a neural network and optionally tracks its performance on a validation set.\n",
    "\n",
    "    Args:\n",
    "        train_data (array-like): The training input data.\n",
    "        train_labels (array-like): The correct labels for the training data.\n",
    "        validation_data (array-like, optional): The validation input data.\n",
    "        validation_labels (array-like, optional): The correct labels for the validation data.\n",
    "        number_of_epochs (int): The number of training epochs.\n",
    "        layer_nodes (list): A list specifying the number of nodes in each layer.\n",
    "        learning_rate (float): The learning rate for weight updates.\n",
    "\n",
    "    Returns:\n",
    "        list: The trained weight matrices of the network.\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_hidden_layers = len(layer_nodes) - 1  # Determine the number of hidden layers\n",
    "    weights = initialize_weights(layer_nodes)  # Initialize weight matrices\n",
    "\n",
    "    for epoch in range(number_of_epochs + 1):\n",
    "        weights = train_neural_network(train_data, train_labels, learning_rate, weights)  # Perform training\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(\"Epoch:\", epoch)\n",
    "            print(\"Training Accuracy:\", calculate_accuracy(train_data, train_labels, weights))\n",
    "            if validation_data is not None and validation_labels is not None:\n",
    "                print(\"Validation Accuracy:\", calculate_accuracy(validation_data, validation_labels, weights))\n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Training Accuracy: 0.3508771929824561\n",
      "Validation Accuracy: 0.38461538461538464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20\n",
      "Training Accuracy: 0.9210526315789473\n",
      "Validation Accuracy: 0.8461538461538461\n",
      "Epoch: 40\n",
      "Training Accuracy: 0.9649122807017544\n",
      "Validation Accuracy: 0.9230769230769231\n",
      "Epoch: 60\n",
      "Training Accuracy: 0.9649122807017544\n",
      "Validation Accuracy: 0.9230769230769231\n",
      "Epoch: 80\n",
      "Training Accuracy: 0.9736842105263158\n",
      "Validation Accuracy: 0.9230769230769231\n",
      "Epoch: 100\n",
      "Training Accuracy: 0.9473684210526315\n",
      "Validation Accuracy: 0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of features and outputs\n",
    "num_of_features = len(X[0])\n",
    "num_of_outputs = len(Y[0])\n",
    "\n",
    "# Define the architecture of the neural network\n",
    "layer_sizes = [num_of_features, 5, 8, num_of_outputs]\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.15\n",
    "epochs = 100\n",
    "\n",
    "# Train the neural network using the specified parameters\n",
    "trained_weights = neural_network(X_train, Y_train, X_val, Y_val, epochs, layer_sizes, learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy:\", calculate_accuracy(X_test, Y_test, trained_weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
