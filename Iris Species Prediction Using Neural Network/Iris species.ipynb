{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>122</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>85</td>\n",
       "      <td>5.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>93</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>116</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  \\\n",
       "0    115            5.8           2.8            5.1           2.4   \n",
       "1     61            5.0           2.0            3.5           1.0   \n",
       "2    129            6.4           2.8            5.6           2.1   \n",
       "3     70            5.6           2.5            3.9           1.1   \n",
       "4    140            6.9           3.1            5.4           2.1   \n",
       "..   ...            ...           ...            ...           ...   \n",
       "145  122            5.6           2.8            4.9           2.0   \n",
       "146   85            5.4           3.0            4.5           1.5   \n",
       "147   93            5.8           2.6            4.0           1.2   \n",
       "148    2            4.9           3.0            1.4           0.2   \n",
       "149  116            6.4           3.2            5.3           2.3   \n",
       "\n",
       "             Species  \n",
       "0     Iris-virginica  \n",
       "1    Iris-versicolor  \n",
       "2     Iris-virginica  \n",
       "3    Iris-versicolor  \n",
       "4     Iris-virginica  \n",
       "..               ...  \n",
       "145   Iris-virginica  \n",
       "146  Iris-versicolor  \n",
       "147  Iris-versicolor  \n",
       "148      Iris-setosa  \n",
       "149   Iris-virginica  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv(\"Iris.csv\")\n",
    "# randomize the data\n",
    "iris = iris.sample(frac=1).reset_index(drop=True)\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8, 2.8, 5.1, 2.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [6.9, 3.1, 5.4, 2.1]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris[['SepalLengthCm','SepalWidthCm', 'PetalLengthCm','PetalWidthCm']]\n",
    "# converting into numpy array\n",
    "X = np.array(X)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot_encode = pd.concat([iris, pd.get_dummies(iris['Species'], prefix = 'Species')], axis = 1)\n",
    "# hot_encode = hot_encode.drop('Species', axis = 1)\n",
    "# hot_encode[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ashif\\anaconda3\\envs\\python_3.12\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Create an encoder to convert categorical labels to one-hot encoded vectors\n",
    "one_hot_encoder = OneHotEncoder(sparse = False)  # Explicitly set sparse to False for dense output\n",
    "\n",
    "# Extract the species labels from the iris dataset\n",
    "Y = iris.Species\n",
    "\n",
    "# Learn the unique labels and transform the data into one-hot encoded vectors\n",
    "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1,1))\n",
    "\n",
    "# Print the first 5 rows of the encoded labels\n",
    "Y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### spliting data into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.15)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights\n",
    "\n",
    "**1. Function Definition:**\n",
    "\n",
    "- `def initialize_weights(node_counts):`: Defines a function named `initialize_weights` that takes a list of node counts as input.\n",
    "\n",
    "**2. Function Purpose:**\n",
    "\n",
    "- The function initializes weights for a neural network with random values between -1 and 1.\n",
    "\n",
    "**3. Arguments:**\n",
    "\n",
    "- `node_counts`: A list containing the number of nodes in each layer of the neural network.\n",
    "\n",
    "**4. Returns:**\n",
    "\n",
    "- A list of weight matrices, one for each layer (except the input layer), representing the connections between nodes in adjacent layers.\n",
    "\n",
    "**5. Code Steps:**\n",
    "\n",
    "- **Calculate Number of Layers:**\n",
    "   - `number_of_layers = len(node_counts)`: Determines the total number of layers in the network based on the length of the `node_counts` list.\n",
    "- **Initialize List for Weights:**\n",
    "   - `weights = []`: Creates an empty list to store the generated weight matrices.\n",
    "- **Loop Through Layers (Except Input):**\n",
    "   - `for layer_index in range(1, number_of_layers):`: Iterates through each layer, starting from the second layer (index 1) to avoid the input layer.\n",
    "      - `current_layer_nodes = node_counts[layer_index]`: Retrieves the number of nodes in the current layer.\n",
    "      - `previous_layer_nodes = node_counts[layer_index - 1]`: Retrieves the number of nodes in the previous layer.\n",
    "      - **Create Weight Matrix:**\n",
    "         - `weight_matrix = np.random.uniform(-1, 1, size=(current_layer_nodes, previous_layer_nodes + 1))`: Creates a matrix of random values between -1 and 1, with dimensions matching the number of nodes in the current and previous layers. The extra column is for bias weights.\n",
    "         - `weight_matrix = np.matrix(weight_matrix)`: Converts the matrix to a NumPy matrix for compatibility.\n",
    "      - **Append Weight Matrix:**\n",
    "         - `weights.append(weight_matrix)`: Adds the generated weight matrix to the `weights` list.\n",
    "- **Return Weights:**\n",
    "   - `return weights`: Returns the list of weight matrices as the function's output.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- This function is essential for initializing weights in neural networks before training.\n",
    "- It ensures that weights start with random values to avoid bias and allow the network to learn during training.\n",
    "- The weights are crucial for determining the strength of connections between nodes and how signals propagate through the network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(node_counts):\n",
    "    \"\"\"\n",
    "    Initializes weights for a neural network with random values between -1 and 1.\n",
    "\n",
    "    Args:\n",
    "        node_counts (list): A list containing the number of nodes in each layer.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of weight matrices, one for each layer (except the input layer).\n",
    "    \"\"\"\n",
    "\n",
    "    number_of_layers = len(node_counts)\n",
    "    weights = []\n",
    "\n",
    "    # Create weight matrices for each layer (except the input layer)\n",
    "    for layer_index in range(1, number_of_layers):\n",
    "        current_layer_nodes = node_counts[layer_index]\n",
    "        previous_layer_nodes = node_counts[layer_index - 1]\n",
    "\n",
    "        # Initialize weights with random values\n",
    "        weight_matrix = np.random.uniform(-1, 1, size=(current_layer_nodes, previous_layer_nodes + 1))\n",
    "\n",
    "        # Convert to a NumPy matrix for compatibility\n",
    "        weight_matrix = np.matrix(weight_matrix)\n",
    "\n",
    "        weights.append(weight_matrix)\n",
    "        \n",
    "\n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe weight matrix will look something like this:\\n\\nWeight Matrix 1: (6, 5)\\n[[ 0.123  -0.456   0.789   0.321   0.234 ] \\n [ 0.567  -0.678  -0.123   0.876   0.456 ]\\n [ 0.789   0.987  -0.234  -0.567  -0.789 ]\\n [-0.987   0.654   0.321  -0.432   0.012 ]\\n [ 0.345  -0.789   0.876   0.234  -0.567 ]\\n [ 0.123   0.456  -0.678   0.789   0.321 ]]\\n\\nWeight Matrix 2: (9, 6)\\n[[ 0.234  -0.567   0.789   0.432   0.012   0.876   0.789  -0.123   0.567 ]\\n [-0.789   0.345  -0.876   0.234   0.567  -0.678  -0.987   0.321  -0.234 ]\\n [ 0.876   0.123   0.456   0.789   0.321   0.234  -0.567  -0.789   0.987 ]\\n [ 0.567  -0.678   0.789  -0.987   0.654   0.321  -0.432   0.012   0.345 ]\\n [-0.234   0.789  -0.321   0.876  -0.789   0.234   0.456   0.678  -0.876 ]\\n [ 0.987  -0.432  -0.567   0.012   0.345  -0.678   0.789  -0.123   0.567 ]\\n [ 0.456  -0.876   0.234  -0.567   0.789   0.321  -0.234   0.987  -0.789 ]\\n [ 0.321   0.234  -0.567   0.789   0.987  -0.234   0.456  -0.789   0.876 ]\\n [-0.678   0.789   0.012   0.345  -0.876   0.567   0.234  -0.789   0.123 ]]\\n\\nWeight Matrix 3: (3, 9)\\n[[ 0.987  -0.234   0.456   0.789  -0.321   0.012   0.345  -0.678   0.876 ]\\n [ 0.567   0.789  -0.876   0.234   0.321  -0.987  -0.678   0.123   0.456 ]\\n [-0.789   0.234   0.567  -0.876   0.987   0.012   0.345  -0.678   0.876 ]]\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The weight matrix will look something like this:\n",
    "\n",
    "Weight Matrix 1: (6, 5)\n",
    "[[ 0.123  -0.456   0.789   0.321   0.234 ] \n",
    " [ 0.567  -0.678  -0.123   0.876   0.456 ]\n",
    " [ 0.789   0.987  -0.234  -0.567  -0.789 ]\n",
    " [-0.987   0.654   0.321  -0.432   0.012 ]\n",
    " [ 0.345  -0.789   0.876   0.234  -0.567 ]\n",
    " [ 0.123   0.456  -0.678   0.789   0.321 ]]\n",
    "\n",
    "Weight Matrix 2: (9, 6)\n",
    "[[ 0.234  -0.567   0.789   0.432   0.012   0.876   0.789  -0.123   0.567 ]\n",
    " [-0.789   0.345  -0.876   0.234   0.567  -0.678  -0.987   0.321  -0.234 ]\n",
    " [ 0.876   0.123   0.456   0.789   0.321   0.234  -0.567  -0.789   0.987 ]\n",
    " [ 0.567  -0.678   0.789  -0.987   0.654   0.321  -0.432   0.012   0.345 ]\n",
    " [-0.234   0.789  -0.321   0.876  -0.789   0.234   0.456   0.678  -0.876 ]\n",
    " [ 0.987  -0.432  -0.567   0.012   0.345  -0.678   0.789  -0.123   0.567 ]\n",
    " [ 0.456  -0.876   0.234  -0.567   0.789   0.321  -0.234   0.987  -0.789 ]\n",
    " [ 0.321   0.234  -0.567   0.789   0.987  -0.234   0.456  -0.789   0.876 ]\n",
    " [-0.678   0.789   0.012   0.345  -0.876   0.567   0.234  -0.789   0.123 ]]\n",
    "\n",
    "Weight Matrix 3: (3, 9)\n",
    "[[ 0.987  -0.234   0.456   0.789  -0.321   0.012   0.345  -0.678   0.876 ]\n",
    " [ 0.567   0.789  -0.876   0.234   0.321  -0.987  -0.678   0.123   0.456 ]\n",
    " [-0.789   0.234   0.567  -0.876   0.987   0.012   0.345  -0.678   0.876 ]]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return np.multiply(x, 1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward\n",
    "\n",
    "**1. Function Definition:**\n",
    "\n",
    "- `def feed_forward(inputs, weights, number_of_layers):`: Defines a function named `feed_forward` that simulates the feedforward process in a neural network.\n",
    "\n",
    "**2. Function Purpose:**\n",
    "\n",
    "- The function calculates the activations of each layer in a neural network, given input values and weight matrices.\n",
    "\n",
    "**3. Arguments:**\n",
    "\n",
    "- `inputs`: The input values provided to the network.\n",
    "- `weights`: A list of weight matrices representing the connections between layers.\n",
    "- `number_of_layers`: The total number of layers in the network.\n",
    "\n",
    "**4. Returns:**\n",
    "\n",
    "- A list of activation values for each layer, reflecting the output of each layer's neurons during the feedforward process.\n",
    "\n",
    "**5. Code Steps:**\n",
    "\n",
    "- **Initialize Variables:**\n",
    "   - `activations = [inputs]`: Creates a list to store the activations of each layer, starting with the input values as the activations of the first layer.\n",
    "   - `current_input = inputs`: Sets the initial input for the first layer.\n",
    "- **Iterate Through Layers:**\n",
    "   - `for layer_index in range(number_of_layers):`: Iterates through each layer in the network.\n",
    "      - `current_weights = weights[layer_index]`: Retrieves the weight matrix for the current layer.\n",
    "      - **Calculate Layer Activation:**\n",
    "         - `layer_activation = sigmoid(np.dot(current_input, current_weights.T))`:\n",
    "           - Calculates the weighted sum of inputs and weights using matrix multiplication (`np.dot`).\n",
    "           - Applies the sigmoid activation function to the weighted sum, resulting in the activation values for the current layer's neurons.\n",
    "      - **Append Bias Term:**\n",
    "         - `layer_input_with_bias = np.append(1, layer_activation)`: Adds a bias term (1) to the activations, preparing them as input for the next layer.\n",
    "      - **Store Activations and Prepare for Next Layer:**\n",
    "         - `activations.append(layer_activation)`: Stores the calculated activations for the current layer in the `activations` list.\n",
    "         - `current_input = layer_input_with_bias`: Updates the `current_input` for the next iteration of the loop, using the bias-appended activations as input for the subsequent layer.\n",
    "- **Return Activations:**\n",
    "   - `return activations`: Returns the list of activations for each layer, representing the network's output during the feedforward process.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- This function is central to the forward propagation step in neural network training.\n",
    "- It simulates how signals flow through the network, from input to output, through sequential activations of neurons in each layer.\n",
    "- Understanding this process is essential for comprehending how neural networks process information and make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(inputs, weights, number_of_layers):\n",
    "    \"\"\"\n",
    "    Calculates the activations of each layer in a neural network during the feedforward process.\n",
    "\n",
    "    Args:\n",
    "        inputs (array-like): The input values to the network.\n",
    "        weights (list): A list of weight matrices for each layer.\n",
    "        number_of_layers (int): The total number of layers in the network.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of activation values for each layer.\n",
    "    \"\"\"\n",
    "\n",
    "    activations = [inputs]  # Store activations for each layer\n",
    "    current_input = inputs  # Initialize input for the first layer\n",
    "\n",
    "    for layer_index in range(number_of_layers):\n",
    "        current_weights = weights[layer_index]\n",
    "\n",
    "        # Calculate weighted sum and apply activation function\n",
    "        layer_activation = sigmoid(np.dot(current_input, current_weights.T))\n",
    "\n",
    "        # Append bias term for the next layer\n",
    "        layer_input_with_bias = np.append(1, layer_activation)\n",
    "\n",
    "        activations.append(layer_activation)  # Store activations\n",
    "        current_input = layer_input_with_bias  # Prepare input for the next layer\n",
    "\n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActivation list will look something like this:\\n\\nActivation Layer 1: [0.1 0.2 0.3 0.4 0.5]\\n\\nActivation Layer 2: [0.793 0.559 0.853 0.832 0.542 0.802]\\n\\nActivation Layer 3: [0.742 0.675 0.733 0.723 0.778 0.676 0.669 0.654 0.778]\\n\\nActivation Layer 4: [0.831 0.756 0.681]\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Activation list will look something like this:\n",
    "\n",
    "Activation Layer 1: [0.1 0.2 0.3 0.4 0.5]\n",
    "\n",
    "Activation Layer 2: [0.793 0.559 0.853 0.832 0.542 0.802]\n",
    "\n",
    "Activation Layer 3: [0.742 0.675 0.733 0.723 0.778 0.676 0.669 0.654 0.778]\n",
    "\n",
    "Activation Layer 4: [0.831 0.756 0.681]\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
